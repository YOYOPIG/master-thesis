\documentclass{PHlab-thesis}

\addbibresource{thesis.bib}

\newcommand*\Department中文{資訊工程學研究所}
\newcommand*\Department英文{Institute of Computer Science and Information Engineering}

\newcommand*\ThesisTitle中文{EAGLE-GPU：使用圖形處理單元加速替代基因組可能性評估}
\newcommand*\ThesisTitle英文{EAGLE-GPU: Acceleration of Alternative Genome Likelihood Evaluation using Graphics Processing Unit}
%\newcommand*\ThesisNote中文{示例:其實徐翡曼是東京大學畢業的博士}% For real thesis omit, or use {初稿} etc.
%\newcommand*\ThesisNote英文{Just an example.  Fei-Man actually graduated from Tokyo Univ.}% For real thesis omit, or use {draft} etc.

\newcommand*\Student中文{盧宥霖}
\newcommand*\Student英文{You-Lin Lu}

\newcommand*\Advisor中文{賀保羅}
\newcommand*\Advisor英文{Paul Horton}

%% 果有共同指導老師可以用:
%% \newcommand*\CoAdvisorA中文{}
%% \newcommand*\CoAdvisorA英文{}
%% \newcommand*\CoAdvisorB中文{}
%% \newcommand*\CoAdvisorB英文{}


\newcommand*\YearMonth英文{July, 2022}
\newcommand*\YearMonth中文{１１１年７月}

\pagestyle{fancy}
\begin{document}


\newcommand*\Keywords英文{genomics, variant calling, GPU acceleration}
\newcommand*\Abstract英文{%TODO
We introduce MethylSeqLogo, an extension of sequence logos to vizualize DNA methylation.
}


\newcommand*\Keywords中文{基因組、字串演算法}
\newcommand*\Abstract中文{%TODO
MethylSeqLogo...衍伸sequence logo的視覺化方法改善包括DNA甲基化的資訊。
}

\newcommand*\Acknowledgements{%
Thank you Prof. Horton%
}



\input{frontmatter}% 封面頁, 口委中英文簽名單, 誌謝, 中英文摘要, 論文目錄, 圖表目錄


%────────────────────  List of Symbols  ────────────────────
\renewcommand\nomgroup[1]{%
  \item[\bfseries
  \ifstrequal{#1}{A}{General}{%
  \ifstrequal{#1}{Z}{Gene/Protein Names}%
  }]}

\nomenclature[A]{$\lg$}{Logarithm base 2}
\nomenclature[A]{KL\ Divergence}{Kullback-Liebler Divergence}
\nomenclature[Z]{Myc}{MYC proto-oncogene}
\nomenclature[Z]{USF-1}{Upstream stimulatory factor 1}

\printnomenclature[5cm]

\newpage
\setcounter{page}{1}
\pagenumbering{arabic}


\chapter{Introduction}
Next generation sequencing has played a significant role in bioinformatics research for the past decades~\cite{behjati2013next,schuster2008next}. This technology enables sequencing the whole human genome sequence in a relative short amount of time, allowing more advanced biological analysis and even medical applications~\cite{roukos2010next}. For example, numerous genetic and epigenetic mechanisms, such as DNA mutation and methylation, could be scrutinized if the accurate genome sequencing data is available~\cite{moore2013dna}. Better knowledge of the human genome system thus makes great impact to clinical practices, notably the diagnosis of hereditary diseases and those related to genetic disorders~\cite{shashi2014utility,stenson2017human}, including specific subtypes of cancer~\cite{serrati2016next}.

Despite the breakthroughs made by utilizing NGS results, several underlying obstacles, however, still exists in the sequencing pipeline. Since sequencers are unable to produce reads as long as the whole human genome sequence without errors, the sequencing results often come in as multiple shorter read segments, leading to that analysis and data processing should be done in advance for researchers to gain insights~\cite{muzzey2015understanding}. Among them, variant calling remains to be a challenging subtask. Various methods have been proposed for this specific task, putting sight on increasing the accuracy of the called variant results. For instance, the Genome Analysis Toolkit, often known as GATK~\cite{mckenna2010genome}, and SAMtools~\cite{li2009sequence}, are some of the most popular tools for this certain task. Notwithstanding the efforts put in this problem, though, the results of those often turn out to have a rather low  precision-recall score, while disagreements were encountered between outcomes from different variant calling pipeline~\cite{o2013low}. In order to justify whether the called variant is likely to happen, there are a number of methods proposed to systematically assess the results~\cite{hwang2015systematic,yu2013comparing}. Moreover, there are also several methods proposed, acting as an additional process to the variant calling pipeline, to revise the called variants and boost the accuracy to some extent. They were shown to improve the results of the called variants after being applied at certain stages in the pipeline. This includes the predecessor of this study, EAGLE: Explicit Alternative Genome Likelihood Evaluator~\cite{kuo2018eagle}, published in 2017 by Tony Kuo et al. It is applied after the variant calling phrase, inspecting each of the called variant by evaluating their likelihood, according to the given reference genome. In comparison to the originally called variants, it is demonstrated that such post-processing improves the precision at an acceptible recall rate.

However, we noticed that the execution of EAGLE could take up to a decent amount of time, despite the usage of the provided multithreading option. Although the total execution time seems rather acceptible currently, we would like to further investigate potential oppurtunities of acceleration, regarding the upcoming third generation sequencing, where sequencing read lengths could easily grow up from those of NGS data around 100 base pairs all the way up to 10,000 base pairs. Furthermore, when we tried to make adjustments to the original program for other applications, an upserge in total execution time is repeatedly observed, making it difficult for more complex experiments.

Since sequence alignment is a highly parrallel task, it was already shown in the original EAGLE research that multithreading could effectively speedup the whole process. In order to further accelerate the program, adding more threads would be a straightforward approach. While the total amount of threads that can be ran concurrently is constrained by the hardware, it is often capped around tens to twenties for modern mainstream CPUs. This is far from enough in comparison to human genome sequences, motivating us to search for other sources of acceleration, including the usage of additional devices. 

This is when Graphics Processing Units, usually abbreviated as GPUs, come to our mind. Originally designed for graphics rendering, GPUs were first dedicated to graphics rendering pipeline. They are responsible for tasks such as vertex and fragment shading, rasterization, etc. The results were then mapped to the output devices, usually a computer monitor, where they are assembled to compute the value of each pixel, before the frame is presented to the user of the computer~\cite{foley1994introduction,owens2008gpu}. The job, being very computational expensive, depends heavily on the throughput that can be produced by the processors. Thus, the designation of GPU is mainly focused on the total amount of data throughtput, aiming to provide as many frames per second as possible, in order to provide a smooth screen that results in a better user experience. There exists multiple ways to achieve better data throughtput, applicable for different situations. When it comes to graphics rendering, which is a highly parallel task to compute values of RGB channels for each pixel presented to the screen, enabling more threads to execute the instructions concurrently is definitely a favorable direction. Hence, modern GPUs come with a lot more cores in comparison to CPUs. Although the computing power of a single GPU core is less than that of a single CPU core, with the total amount of cores outnumbering CPUs by a lot, plentiful of threads can be run at the same time due to having its own physical core, which is highly beneficial for data-parallel problems~\cite{navarro2014survey}. 

In spite of the dedication to graphics rendering when first developed, GPUs have gained their popularity towards general purpose computing. Equipped with enormous amount of physical cores, GPUs are also found suitable for single instruction, multiple data (SIMD) missions. With the size of data to be processed continuously thriving in the recent years, even simple instructions began to become computationally demanding. People started to look upon the utilization of GPU's high-throughput architecture, seeking for the possibility to speed up time consuming jobs~\cite{owens2008gpu,nickolls2010gpu}. Although programming on GPUs were not straightforward due to quite a few limitaions since they were used to tackle rendering problems in the past and programmability was never a main concern, NVIDIA, one of the main GPU designers, developed the well known CUDA architecture, acting as a integrated parallel computing platform with the application programming interface that comes collectively. This grants programmers an easier access to the GPU devices, encouraging them to explore the feasibility of parallel computation on GPUs and other sources of general purpose computation. In the recent years, GPUs have been shown to greatly accelerate the process of a variety of tasks, including computer vision, deep learning, etc. Thus, we would like to investigate whether sequence alignment and variant calling, composed of several subtasks which are also highly data-parallel, can be accelerated with the aid of GPUs.

Here, we present a EAGLE-GPU, a GPU accelerated version of EAGLE. To better integrate with the previous toolchain, most of the probablistic model would be rather similar to the original method, while several adjustments were made to reduce memory copy between device and host, making it a better fit to the device-host parellel computing architecture. This makes EAGLE-GPU adaptive for users from the previous program, allowing them to toggle between using the CPU entirely or enabling GPU acceleration, according to the characteristic of the input data and the availability of GPU devices. Experiments were then made by using both real data and simulated data, demonstrating that accelerating the likelihood evaluation process is beneficial when facing a massive sequencing read data.

\chapter{Related Works}
In this chapter, we would like to first review recent works related to GPU acceleration applied to bioinformatics in brief. With regards to this research being a follow up work of EAGLE, we would also take a closer look at the underlying probablistic model behind the original method, which would later be used and modified to run on the GPU device.
\section{Bioinformatics with GPU}
\subsection{NVBIO}
The high throughput strength of GPU has already been applied to the field of bioinformatics. NVIDIA released their own GPU-based library, NVBIO, for sequence alignment problems~\cite{nvbio2015}. In virtue of the fine tuned functions in the library, users are supported to program in a relatively high level fashion, without needing to handle issues about physical GPU device access. This helps alleviate the frustration programmers, especially those who used to write code that runs exclusively on CPUs, often face with when encountering GPU related low level operations. Aside from libraries for programmers to integrate in their bioinformatics project, NVBIO accommodates several applications which were already built on top of the massively parallel architecture. Multiple well known sequence alignment algorithms were re-engineered in a high parallel manner, implemented with the previously developed library. For example, Bowtie2~\cite{langmead2012fast}, a gapped read alignment tool, is recreated as nvBowtie in the package.
\subsection{Others works}
Numerous other applications in bioinformatics were also shown to be advantageous. Yung et. al presented GBOOST~\cite{yung2011gboost} for gene-gene interaction, while an influential sequence analysis tool, BLAST~\cite{altschul1990basic}, were implemented with parallel computing based on GPU and achieved around 2 to 4 times speedup~\cite{vouzis2011gpu}. 

\section{EAGLE}
Critical concepts of the previous work EAGLE would be recapitulated in this section. First of all, three files would be provided as input: the called variants in a vcf file, the reference genome, and the aligned sequencing reads in a bam file format. After processing the data given, the called variants are then evaluated by their likelihood. With the given data, we could obtain the quality of called variants by examining whether the reads better support the hypothesis which rather than the reference genome, they were more likely to be sequenced from an alternative sequence consisting of the called variants, indicating a higher probability that the called variants are actually present. On the other hands, if the reads better suit the other hypothesis that they are not related to the called variants and were more possibly sequenced from the reference genome, we could then lower our confidence level on the called variant. Therefore, we would have to calculate the probability $P[r|G]$ regularly over all sequencing reads $r$ in a read set covering the interval with interest, while $G$ denotes the hypothetical genome sequence, either one of the reference genome $G_ref$ or the alternative genome, $G_alt$. For each iteration calculating $P[r|G]$, we can derive $P[r|G]$ as the summation of the probability having $r$ being sequenced from $g$, a certain genome segment in $G$ with the same length $l_r$ as the read $r$, over all possible $g$s. Such equation can be written as follows:
\begin{equation*}
P[r|G] = \sum_{g\in G} P[g|G] P[r|g]
\end{equation*}
where $P[g|G]$ is the probability that genome sequence segment $g$ is sequenced, with respect to that some segment from $G$ is sequenced. In the original work, two assumptions were made here. Firstly, uniform priors were assumed on genome segments $g$ of any particular length. Secondly, to acquire the probability that the genome segment were having the same length as the read, available for further computation, the length distribution of $g$ is assumed to follow the general prior defined as $\mathcal{L}(\ell)$. That is,
%%
\begin{equation*}
P[g|G] = \mathcal{L}(\ell_g) \frac{1}{n_{\ell_g}}
\end{equation*}
having the symbol $n_{\ell_g}$ representing the number of genome segments of length $\ell_g$.
Now there is $P[r|g]$ remaining in the previous equation. Regarding that it is difficult to compute directly, Bayes' Law could be applied, leading to
\begin{equation*}
P[r|g] = \frac{P[r]}{P[g]} P[g|r]
\end{equation*}
Here, $P[g]$ denotes the prior probability of sequence $g$ given that $g$ was read by a sequencer, considering possible errors when sequencing. However, an assumption that no indel errors happened was made back then, assuring that the length of the sequence did not change during the process. The prior probability distribution of the length of reads, denoted as $\mathcal{L}(\ell)$, is resolved during the experiment. For DNA sequencing, there are 4 potential options in the set \texttt{\{a,c,g,t\}} for each base, with $\ell_g$ bases to be considered, which is,
\begin{equation*}
P[g] =  \frac{\mathcal{L}(\ell_g)}{4^{\ell_g}}
\end{equation*}
Here, since $P[r]$ is going to be cancelled later on, it will be left as is. Aware of sequencing errors, the computation of $P[g|r]$ is performed by calculating the product of the probabilities for all bases, considering the base call quality score. Thus,
\begin{equation}
P[g|r] = \prod_{i=1}^{\ell_r} P[g_i|r_i].
\end{equation}
where $g_i$ is the $i$th base of $g$ and $r_i$ is the probability vector over \texttt{\{a,c,g,t\}} corresponding to the base and quality score of the $i$th position of $r$. The above equations, can further derive
\begin{equation*}
\begin{split}
P[r|G] =  \sum_{g\in G} P[g|G] P[r|g] &= \sum_{g\in G} \left( \mathcal{L}(\ell_g) \frac{1}{n_{\ell_g}} \right) \frac{P[r]4^{\ell_g}}{\mathcal{L}(\ell_g)} \prod_{i=1}^{\ell_g} P[g_i|r_i]  \\
                                      &= \sum_{g\in G} \frac{P[r]4^{\ell_g}}{n_{\ell_g}} \prod_{i=1}^{\ell_g} P[g_i|r_i]  \\
                                      &= \sum_{g\in G} \frac{P[r]}{n_{\ell_g}} \prod_{i=1}^{\ell_g} 4P[g_i|r_i]  \\
                                      &\approx \frac{P[r]}{n} \sum_{g\in G} \prod_{i=1}^{\ell_g} 4P[g_i|r_i]  \\
\end{split}
\end{equation*}

This is where the iterative loops of data parallel operations were executed. Focusing on reducing the time required to complete the above task, we will describe the method adapted in the following chapter.

\chapter{Method}
\section{Method Overview}
In the original EAGLE publication, whether a hypothesis fits the sequenced read data is determined by its likelihood, often measured by its ratio against the default hypothesis: the read being sequenced from the reference genome. In order to compute the likelihood for all possible read alignments, the probabilities of all read and genome segment pair are calculated and then further summed up in logarithm. Although this is undoubtly a straightforward solution, numerous time consuming operations were done in loop. Here, we unroll the nested loops and take advantage of the high parallel graphics processing units. 

\begin{figure}[positioning value]
  \centering
  \includegraphics[width=6cm]{figure/overview.png}
  \caption{caption for this figure}
  \label{fig:label for this figure}
\end{figure}

As shown in figure 1, our goal is to parallelize the parts circled, with the attempt to reduce the total amount of time required to execute the program. We will go through the details in the following section.

\section{Proposed Scheme}
\subsection{CUDA programming model}
Before all else, we should first cautiously design our workflow, trying to acclimate to the CUDA programming model. There exists several important points to be aware of, which could affect the performance significantly because of the hardware implementations of GPUs. For modern NVIDIA GPUs that support CUDA, often comes with numerous streaming processors(SP), which are the cores on the GPU. Each of them is responsible for executing a thread in our program. They are grouped to form streaming multiprocessors(SM), the basic unit programmers control to execute our parallelized functions, also known as kernels in CUDA terminology. Streaming multiprocessors have their own set of streaming processors, along with their own hardware units such as registers, cache, shared memory, and warp scheduler that schedules warps, the basic unit when it comes to runtime execution. Each warp contains 32 threads, while they share one program counter. Thus, when threads in the same warp executes instructions conditionally, possibly through statements such as if-else, will slow down the process since those threads not executing will be temporarily deactivated, waiting for others to finish. Such problem is refferd to as warp divergence, and could result in a performance loss due to the serialized manner. Another issue that could worsen performance is memory copying. Since kernels launced on GPUs, or device for CUDA terminology, cannot directly access the host memory, those data to be computed should be copied to the device beforehands. However, copying data between host and device is quite time consuming, being likely to take more time than the actual computing operations if not designed carefully. Hence, these are our main concerns during development, while seeking to keep the kernels generalized, ready for further integration. To better demonstrate how the task is designed in a parallel fashion, the pseudocode of the implementation, without parallel computing, is shown below.

We will then describe how we built up the parallelized implementation step by step in the following sections.

\subsection{Computing the likelihood with given genome segment}
First of all, for any given pair of genome segment g and sequence read r, the likelihood of the read being sequenced from g is computed by product of the likelihood of all base pairs. Since the likelihood of each bp match is independant to other bp matches, they can be computed concurrently. This provides us the oppurtunity to accelerate this process by computing them in parallel, where the maximum number of operations that can be done in parallel is bounded by the length of the read r. Considering NGS read data, which is often around 100 to 150 base pairs in terms of read length, we can easily load the entire read onto the GPU and allocate a thread for each base, since the size is rather small in comparison to the memory capacity and the core counts of modern GPUs.
\subsection{Computing the likelihood with given reference genome}
Next, we would like to further extend the degree of parallelism, seeking to utilize the performance provided by the GPU. Here, we take a closer look at the intermediate result, $p[r|G]$. In most cases, knowing the exact genome segment g where read sequence r is sequenced from in advance is unrealistic. Thus, in the original method, for all possible genome segment g sampled from the hypothesis genome G, with the exact length with the given read r, were considered. $P[r|g]$ are calculated iteratively, with the summation of their results being the final score of the given read. Since the total amount of segments to be considered is related to the length of read sequence(=2*lr), the execution time required grows in polynomial time with the sequence length. Apparently, the results of P[r|g] for different genome segment g is also independant, opening an oppurtunity for parallelism. In addition to the entire read r, we also loaded the selected genome G to the GPU, allocating a block of threads for each possible genome segment g. Within each block, we execute the same instructions as described in the previous section, calculating the probabilities for each site in their corresponding thread.

Through the above method, we have already reduced the amount of time required to execute the program. This also allows us to revisit a term that was previously ignored: insertion/deletion errors in read sequences. Since indel errors not only modifies the sequence but also introduce shifts, it would increase the time complexity, proportional to the length of reads. With the previous implementation, where the operations were sequentially executed, this would sure intensify the computational cost and lead to longer execution time. Mainly considering generation sequencing data, which has a rather low probability of indel errors in the sequencing phase, an assumption that there was no indel sequencing errors was made, in order to avoid the problem descibed above. However, with the aid of GPUs, such operations can be done concurrently, reducing the amount of time required to compute the final result. In GPU-EAGLE implementation, a threshold value is input by the user, indicating the maximum number of indel errors to be taken into account. After all, the probability of such errors is still fairly low, making it pointless to compute all of the combinations of shifted reads and indels. Knowing the threshold beforehand, all of the possible reads having less indel errors than it is then listed out and assigned to the GPU. Similar to the base case where no indel errors was considered, we would need a block of threads for each pair of read r and genome segment g. With regard to the same read having multiple variations due to the introduced indel errors, the GPU would launch a two dimensional block instead of the original one dimensional block, with the extra dimension representing the index of read in the list of possible reads with indel. To allow such indeled read list to be long exceeding the block size launched, either because of hardware constraints or user settings, the list would be partitioned to batches with the same size as the y dimensional size of the two dimensional block. At last, the results of each batch would be added up to acquire the final result.

\chapter{Results}
\section{Input dataset}
To better compare the results with and without GPU acceleration, an experiment was conducted using the reconstructed diploid sequence of human genome chromosome 22 and hg19 reference sequence, which is one of the dataset that were used in the original study. Intend to investigate possible speedup when facing third generation sequencing read data, simulated sequencing reads with 10,000 base pairs were generated by random sampling from the reference genome. Reads with different lengths were also generated as a reference and an indicator for possible future generation sequencers.

\section{Benchmark platforms}
EAGLE-GPU is compiled with CUDA version 10.1. The experiments were conducted on a machine with a single AMD Ryzen™ 9 5950X desktop processor, consisting of 16 physical cores running at their base clock 3.4 gigahertz, and 128 GB of random access memory. The GPU installed on the machine is a single NVIDIA GeForce RTX™ 3090 graphics card.

\section{NS12911 variants with simulated NGS reads}
First of all, we used the exact same variant dataset to inspect the performance difference between the two implementations.

\section{Simulated reads with different lengths}

\section{Considering read indel errors}

\chapter{Discussion}
Discussion the significance or your results.
\section{Future Work}
Trying to investigate various possibilites of application for GPU to improve the performance, this paper provides the implementations of the general usage functions in CUDA. Unlike programs executed particularly on CPU, the design of GPU kernels often require extra attention, including memory copying schema and the detail implementation of algorithms. If we peurely consider datasets with shorter sequencing reads, the extent of parallelism could be further increased by computing different read sets concurrently. With a specific kernel for each case, warp divergence could be eliminated. Theoretically, this could benefit cases when the dataset consist of large amount of read sets, enormously reducing the amount of time to evaluate the likelihood by computing the results for each set all at once. Another possible optimization would be lessening the memory copying cost. Despite it not being the bottleneck of the program, some advanced techniques could be applied to lower the amount of data copied between the host CPU and the device even more. For example, if we are targeting only DNA and RNA sequences but not proteins, we could easily represent the bases, A, C, G, T(U for RNA), N, in 3 bits. Thus, instead of copying them as characters, they could be encoded first, enabling us to pack 2 bases into one unsigned integer. This is a common trick among sequencing related GPU applications, also adopted by other tools such as GASAL2 and NVBIO.

\chapter{Conclusion}
In this paper, we propose and implemented a GPU accelerated version of EAGLE, a previously proposed method for alternative genome likelihood evaluation. GPU-EAGLE utilizes the massively parallel architecture of modern GPUs, enabling instructions to be executed concurrently on multiple different data. This prepares the original method for the continuous growth when it comes to the size of sequencing data. Besides, we also investigated the feasibility of taking sequencing read indel errors into consideration, which is computational expensive and used to be ignored, after increasing the program throughput by applying parallel computation. We provide experimental results showing that GPU-EAGLE outperforms the CPU version when the data size is large enough, and conclude that designing a specialized GPU kernel could always be an option for forthcoming applications with excessive computational operations.

\newpage
\AddToContents{References}
\printbibliography


\end{document}
