\documentclass{PHlab-thesis}
\usepackage{amsmath}
\usepackage{amsfonts} 
\usepackage{graphicx}
\usepackage{algpseudocode}
\usepackage{algorithm}
\addbibresource{thesis.bib}

\newcommand*\Department中文{資訊工程學研究所}
\newcommand*\Department英文{Institute of Computer Science and Information Engineering}

\newcommand*\ThesisTitle中文{EAGLE-GPU：使用圖形處理單元加速計算基於 DNA 測序數據之推定基因組變異的統計分析}
\newcommand*\ThesisTitle英文{EAGLE-GPU： Using Graphics Processing Units to accelerate computation of the statistical support of putative genome variants based on DNA sequencing data}
%\newcommand*\ThesisNote中文{示例:其實徐翡曼是東京大學畢業的博士}% For real thesis omit, or use {初稿} etc.
%\newcommand*\ThesisNote英文{Just an example.  Fei-Man actually graduated from Tokyo Univ.}% For real thesis omit, or use {draft} etc.

\newcommand*\Student中文{盧宥霖}
\newcommand*\Student英文{You-Lin Lu}

\newcommand*\Advisor中文{賀保羅}
\newcommand*\Advisor英文{Paul Horton}

%% 果有共同指導老師可以用:
%% \newcommand*\CoAdvisorA中文{}
%% \newcommand*\CoAdvisorA英文{}
%% \newcommand*\CoAdvisorB中文{}
%% \newcommand*\CoAdvisorB英文{}


\newcommand*\YearMonth英文{July, 2022}
\newcommand*\YearMonth中文{１１１年７月}

\pagestyle{fancy}
\begin{document}


\newcommand*\Keywords英文{genomics, variant calling, GPU acceleration}
\newcommand*\Abstract英文{
Variant calling remains to play an important role in analyzing genome sequence data. A variety of studies have been conducted in the related field, proposing solutions to this specific task. The predecessor of this study, EAGLE: Explicit Alternative Genome Likelihood Evaluator, aimed to further improve the precision of the results of those, by assessing the likelihood of the called variants. It was shown that such technique assuredly enhance the accuracy. However, because of the computational complexity of its algorithm, the amount of time required to execute intensifies as the length of sequenced reads grow. Besides, such issue also acts as an obstacle when building more advanced applications on top of it. In this research, we would like to investigate if graphics processing units, or GPUs, could accelerate the process since the algorithm is highly parallel. Here, we propose EAGLE-GPU, a revision of the original method by rewriting the fundamental computing functions as GPU CUDA kernels. The experimental outcomes demonstrate that although the current usage of GPU parallelism might not be suitable for sequencing data with shorter reads, it is beneficial when the read lengths grow, opening up opportunities for the third generation sequencing. In addition, our work enables more complicated applications to establish on top with, due to the reduced time for the computational excessive operations. To conclude, we recommend applying GPU acceleration if encountering a highly parallel dataset with long reads, and that more specialized CUDA kernels could be designed base on our results for future complex problems.
}


\newcommand*\Keywords中文{基因組、變異位點偵測、平行運算}
\newcommand*\Abstract中文{
變異位點偵測在分析基因組序列數據中扮演相當重要的角色。在相關領域中已有各項研究陸續被提出，為這一特定任務提供可行的解決方案。本研究的前身 EAGLE：顯式替代基因組可能性評估器，旨在通過計算各個被偵測到潛在變異位點的可能性，以統計學方法來進一步提高結果的精確度。根據當時的研究數據，這種技術確實有效提高了準確性，相當適合用來進一步分析、處理各大變異位點偵測工具所產生的結果。然而，由於其演算法在計算上的時間複雜度，執行該程式所需的時間會隨著定序讀取長度的增加而大幅增長。此外，高時間複雜度的特性也讓我們在基於原始程式嘗試構建更高層級的應用程式時，遇到執行效率上的問題。由於我們認為該演算法是可以高度平行處理的，在這項研究中，我們想調查簡稱 GPU 的圖形處理單元，是否可以有效加速該過程。在這裡，我們提出了 EAGLE-GPU，通過將基本計算函式重新實作為 GPU CUDA 內核，透過平行處理底層機率計算的方式進行加速。為了未來的可擴充性，我們目前實作的 GPU 平行程式僅包含最底層的計算部分。依據實驗結果，目前的實作可能不適合對應較短的定序資料，若以次世代定序資料為主要研究對象，需要進一步設計更加專一的 CUDA 內核提高平行性。然而，當序列長度增加時，實驗數據顯示利用 GPU 做加速能大幅地降低所需的執行時間，對於序列長度超過一萬的第三代定序而言，無疑是一大幫助。此外，由於直接在基礎計算上減少了過多的時間消耗，我們提出的這份研究也可以對日後建立更高層級的複雜應用程序帶來幫助。綜合來說，根據這次的研究結果，我們建議在遇到高度並行的數據集時適當的應用 GPU 的輔助以加速程式的執行，並且在遭遇未來的複雜問題時，也可以根據我們的結果針對該問題的特性設計 CUDA 內核，快速有效的產生解決的方案。
}

\newcommand*\Acknowledgements{%
首先我想在這裡感謝我的指導教授賀保羅老師。對於大學期間沒有涉略生物相關基礎知識的我來說，透過老師的幫忙讓我了解基因序列分析常用的技巧與困難，也才衍生出進行這份研究的動機。在討論的過程中，也很感謝老師給予的指教，讓我在面對困難時更快速的找到解決問題的方向。本論文的完成同時還要感謝原論文 EAGLE 的主要作者之一 Tony Kuo，除了和我分享當時研究時的寶貴經驗，還提供了當初實驗的測試資料，使我們在本論文中能更客觀地針對結果進行比較與分析。最後，也要感謝這兩年來在實驗室一起奮鬥過的所有同學們，面對大家普遍陌生的生物資訊領域，彼此教學相長，透過不斷的交流討論更快速的習得各方面的專業知識。
}

\input{frontmatter}% 封面頁, 口委中英文簽名單, 誌謝, 中英文摘要, 論文目錄, 圖表目錄


%────────────────────  List of Symbols  ────────────────────
\renewcommand\nomgroup[1]{%
  \item[\bfseries
  \ifstrequal{#1}{A}{General}{%
  \ifstrequal{#1}{Z}{Gene/Protein Names}%
  }]}

% \nomenclature[A]{$\lg$}{Logarithm base 2}
% \nomenclature[A]{KL\ Divergence}{Kullback-Liebler Divergence}
% \nomenclature[Z]{Myc}{MYC proto-oncogene}
% \nomenclature[Z]{USF-1}{Upstream stimulatory factor 1}

\nomenclature[A]{NGS}{Next Generation Sequencing}
\nomenclature[A]{TGS}{Third Generation Sequencing}
\nomenclature[A]{DNA}{deoxyribonucleic acid}
\nomenclature[A]{CPU}{Central processing unit}
\nomenclature[A]{GPU}{Graphics processing unit}
\nomenclature[A]{EAGLE}{Explicit Alternative Genome Likelihood Evaluator, the predecessor of this research}
\nomenclature[A]{bp}{base pair}

\printnomenclature[5cm]

\newpage
\setcounter{page}{1}
\pagenumbering{arabic}


\chapter{Introduction}
\section{Background}
Next generation sequencing (NGS) has played a significant role in bioinformatics research for the past decades~\cite{behjati2013next,schuster2008next}. This technology enables sequencing the whole human genome sequence in a relatively short amount of time, allowing more advanced biological analysis and even medical applications~\cite{roukos2010next}. For example, numerous genetic and epigenetic mechanisms, such as DNA mutation and methylation, could be scrutinized if accurate genome sequencing data is available~\cite{moore2013dna}. Better knowledge of the variation in human genomes could have a great impact on clinical practice; notably the diagnosis of hereditary diseases and those related to genetic disorders~\cite{shashi2014utility,stenson2017human}, including specific subtypes of cancer~\cite{serrati2016next}.

Despite the breakthroughs made by utilizing NGS results, some computational challenges remain~\cite{muzzey2015understanding}.  DNA sequencing machines are unable to produce reads anywhere near as long as an entire human chromosome, so sequencing data consists of many (often millions) of shorter read segments originating from random fragments of the sequenced genome.  Fortunately a ``human reference genome'' is available, which usually allows computer programs to infer which fragment of the genome a read originated from.  This process however is complicated because: (1) sequence reads contain errors, (2) the human genome is repetitive, so fragments from different positions can look alike, and (3) individual genome differ from the human reference. As a consequence, the concept of read quality is introduced during the sequencing procedure, with its score indicating the probability of a base being misread. If a certain base in the sequenced read is different from the human reference genome, we could simply refer to the read quality, in order to determine whether it is a misread or a real genomic difference, also known as a variant.

Variant calling is the task of inferring differences between a sequenced individual's genome and the reference genome, which is a critical function for the analysis of NGS data. To finish this job, one would usually first collect two input data: the sequenced reads of an individual along with their read quality scores, and the reference genome. Then, sequence alignment is performed to map the sequenced reads to positions of the reference genome where they are most likely to be selected from. Finally, variants are called according to the alignment results, recognizing positions where the aligned reads conflict with the reference genome as potential variants.
Various methods and tools have been developed for this specific task, focusing on increasing the accuracy of the called variant results. For instance, the Genome Analysis Toolkit, often known as GATK~\cite{mckenna2010genome}, and SAMtools~\cite{li2009sequence}, are some of the most popular works for this certain task. Notwithstanding the efforts put in this problem, though, the results of those often turn out to have a rather low precision-recall score, while disagreements were encountered between outcomes from different variant calling pipeline~\cite{o2013low}. In order to justify whether the called variant is likely to happen, there are a number of methods proposed to systematically assess the results~\cite{hwang2015systematic,yu2013comparing}. Moreover, there are also several methods proposed, acting as an additional process to the variant calling pipeline, to revise the called variants and boost the accuracy to some extent. They were shown to improve the results of the called variants after being applied at certain stages in the pipeline. This includes the predecessor of this study, EAGLE: Explicit Alternative Genome Likelihood Evaluator~\cite{kuo2018eagle}, published in 2017 by Tony Kuo et al.  EAGLE is applied after the variant calling phrase, using a probabilitistic model of the sequencing process to compute a likelihood for each candidate variant.  In comparison to the originally candidate variants, they demonstrated that such post-processing can improve precision at an acceptable recall rate.

\section{Motivation \& Research objective}
In a nutshell, this thesis explores the possibility of using Graphics Processing Units (GPU)s to accelerate EAGLE's computation.
The reason is that we noticed that the execution of EAGLE can take a considerable amount of time, even when using the multithreading option provided.  This execution time is still acceptable for the short reads (100--200 bp) provided by currently popular Illumina sequencing platforms.  However the EAGLE implementation was not
designed for upcoming third generation sequencing, where sequencing read lengths can be on the order of 10,000 bps in length.  Also EAGLE implements a method to consider insertion and deletion sequencing errors, but the authors
choose not to evaluate that, partially because they thought it would be too slow to be practical.
Thus we decided to investigate potential opportunities of acceleration.

Sequence alignment is a highly parallel task, and it was already shown in the original EAGLE research that multithreading could effectively speed-up the whole process.  In order to further accelerate the program, adding more threads would be a straightforward approach.  But the total amount of threads that can be ran in parallel is constrained by the hardware, often capped around tens to twenties for modern mainstream CPUs.  This motivating us to search for other sources of acceleration, in particular the use of GPUs.

Originally designed for graphics, GPUs were first dedicated to graphics rendering pipeline.  They are responsible for tasks such as vertex and fragment shading, rasterization, etc.  The results are then mapped to the output devices, usually a computer monitor, where they are assembled to compute the value of each pixel, before the frame is presented to the user of the computer~\cite{foley1994introduction,owens2008gpu}.  This job, being very computational expensive, depends heavily on the throughput that can be produced by the processors.  Thus, the design of GPUs is mainly focused on the total amount of data throughput, aiming to provide as many frames per second as possible, in order to provide a smooth screen that results in a better user experience.  There are multiple ways to achieve better data throughput, applicable for different situations. When it comes to graphics rendering, which is a highly parallel task to compute values of RGB channels for each pixel presented to the screen, enabling more threads to execute the instructions concurrently is definitely a favorable direction. Hence, modern GPUs come with a lot more processing cores in comparison to CPUs. Although the computing power of a single GPU core is less than that of a single CPU core, with the total amount of cores outnumbering CPUs by a lot, many threads can be run at the same time due to having its own physical core, which is highly beneficial for data-parallel problems~\cite{navarro2014survey}.

Despite the dedication to graphics rendering when first developed, GPUs have gained their popularity towards general purpose computing. Equipped with enormous amount of physical cores, GPUs are also found suitable for single instruction, multiple data (SIMD) missions. With the size of data to be processed continuously thriving in the recent years, even simple instructions began to become computationally demanding. People started to look upon the utilization of GPUs' high-throughput architecture, seeking for the possibility to speed up time consuming jobs~\cite{owens2008gpu,nickolls2010gpu}. Although programming on GPUs were not straightforward due to quite a few limitations since they were used to tackle rendering problems in the past and programmability was never a main concern, NVIDIA, one of the main GPU designers, developed the well known CUDA architecture, acting as a integrated parallel computing platform with the application programming interface that comes collectively. This grants programmers an easier access to the GPU devices, encouraging them to explore the feasibility of parallel computation on GPUs and other sources of general purpose computation. In recent years, GPUs have been shown to greatly accelerate the process of a variety of tasks, including computer vision, deep learning, etc. Thus, we would like to investigate whether sequence alignment and variant calling, composed of several subtasks which are also highly data-parallel, can be accelerated with the aid of GPUs.

Here, we present EAGLE-GPU, a GPU accelerated version of EAGLE.  To better integrate with the existing toolchain, we adopted the EAGLE probabilistic model as is; but we did make several adjustments to reduce memory copy between CPU and GPU, making it a better fit to the CUDA device-host parallel computing architecture.  This makes EAGLE-GPU flexible for users from the previous program, allowing them to toggle between using the CPU solely or enabling GPU acceleration, according to the characteristics of the input data and the availability of GPU devices.  We conducted experiments using both newly simulated data and the identical data that the original EAGLE paper used; demonstrating that the likelihood evaluation computation can be signficantly sped up for long reads.

\chapter{Related Works}
In this chapter, we would like to first review a few recent works related to GPU acceleration applied to bioinformatics in brief. With regards to this research being a follow up work of EAGLE, we would also take a closer look at the underlying probabilistic model behind the original method, which would later be used and modified to run on the GPU device.
\section{Bioinformatics with GPU}
\subsection{NVBIO}
The high throughput strength of GPU has already been applied to the field of bioinformatics. NVIDIA released their own GPU-based library, NVBIO, for sequence alignment problems~\cite{nvbio2015}. In virtue of the fine tuned functions in the library, users are supported to program in a relatively high level fashion, without needing to handle issues about physical GPU device access. This helps alleviate the frustration programmers, especially those who used to write code that runs exclusively on CPUs, often face with when encountering GPU related low level operations. Aside from libraries for programmers to integrate in their bioinformatics project, NVBIO accommodates several applications which were already built on top of the massively parallel architecture. Multiple well known sequence alignment algorithms were re-engineered in a high parallel manner, implemented with the previously developed library. For example, Bowtie2~\cite{langmead2012fast}, a gapped read alignment tool, is recreated as nvBowtie in the package.
\subsection{Others works}
Numerous other applications in bioinformatics were also shown to be advantageous with GPUs. Yung et.\ al presented GBOOST~\cite{yung2011gboost} for gene-gene interaction, while an influential sequence analysis tool, BLAST~\cite{altschul1990basic}, were implemented with parallel computing based on GPU and achieved around 2 to 4 times speed-up~\cite{vouzis2011gpu}. Several works, on the other hand, commit to providing GPU libraries with even better performance. GASAL2~\cite{ahmed2019gasal2}, for instance, is another accelerated library for sequence alignment, reported to outperform NVBIO. 

\section{EAGLE}
Critical concepts of the previous work EAGLE would be recapitulated in this section. First of all, three files would be provided as input: the called variant candidates in a vcf format, the reference genome, and the aligned sequencing reads in a bam file format. After processing the data given, the candidate variants are then evaluated by their likelihood. With the given data, we could obtain the quality of candidate variants by examining whether the reads better support the hypothesis which they were sequenced from the reference genome, or rather being more likely to be sequenced from an alternative sequence consisting of the candidate variants, indicating a higher probability that the candidate variants are actually present. On the other hands, if the reads better suit the other hypothesis that they are not related to the candidate variants and were more possibly sequenced from the reference genome, we could then lower our confidence level on the candidate variants. Therefore, we would have to calculate the probability $P[r|G]$ regularly over all sequencing reads $r$ in a read set covering the interval with interest, while $G$ denotes the hypothetical genome sequence, either one of the reference genome $G_{ref}$ or the alternative genome $G_{alt}$, containing called variants. For each iteration calculating $P[r|G]$, we can derive $P[r|G]$ as the summation of the probability having $r$ being sequenced from $g$, a certain genome segment in $G$ with the same length $\ell_r$ as the read $r$, over all possible ${g\in G}$. Such equation can be written as follows:
\begin{equation}
P[r|G] = \sum_{g\in G} P[g|G] P[r|g]
\end{equation}
where $P[g|G]$ marks the probability of a certain genome segment $g$ being sequenced, with respect to that some segment from $G$ is sequenced. In the original work, two assumptions were made here. Firstly, uniform priors were assumed on genome segments $g$ to be in any particular length. Secondly, to acquire the probability that the genome segment were having the same length as the read, available for further computation, the length distribution of $g$ is assumed to follow the general prior defined as $\mathcal{L}(\ell)$. Which is, in mathematical equation,
%%
\begin{equation}
P[g|G] = \mathcal{L}(\ell_g) \frac{1}{n_{\ell_g}}
\end{equation}
having the symbol $n_{\ell_g}$ representing the number of genome segments of length $\ell_g$.
Now there is $P[r|g]$ remaining in the previous equation. Regarding that it is difficult to compute directly, Bayes' Law could be applied, leading to
\begin{equation}
P[r|g] = \frac{P[r]}{P[g]} P[g|r]
\end{equation}
Here, $P[g]$ denotes the prior probability of sequence $g$ given that $g$ was read by a sequencer, considering possible errors when sequencing. However, an assumption that no indel errors happened was made back then, assuring that the length of the sequence did not change during the process. The prior probability distribution of the length of reads, denoted as $\mathcal{L}(\ell)$, is resolved during the experiment. For DNA sequencing, there are 4 potential options in the set \texttt{\{a,c,g,t\}} for each base, with $\ell_g$ bases to be considered, which is,
\begin{equation}
P[g] =  \frac{\mathcal{L}(\ell_g)}{4^{\ell_g}}
\end{equation}
Here, since $P[r]$ is going to be canceled later on, it will be left as is. Aware of sequencing errors, the computation of $P[g|r]$ is performed by calculating the product of the probabilities for all bases, considering the base call quality score. Thus,
\begin{equation}
P[g|r] = \prod_{i=1}^{\ell_r} P[g_i|r_i].
\end{equation}
where $g_i$ denotes the $i$th base of $g$, while $r_i$ represents the probability vector for the $i$th position of $r$, with respect to the read quality score over \texttt{\{a,c,g,t\}}. The above equations can thus further derive
\begin{equation*}
\begin{split}
P[r|G] =  \sum_{g\in G} P[g|G] P[r|g] &= \sum_{g\in G} \left( \mathcal{L}(\ell_g) \frac{1}{n_{\ell_g}} \right) \frac{P[r]4^{\ell_g}}{\mathcal{L}(\ell_g)} \prod_{i=1}^{\ell_g} P[g_i|r_i]  \\
                                      &= \sum_{g\in G} \frac{P[r]4^{\ell_g}}{n_{\ell_g}} \prod_{i=1}^{\ell_g} P[g_i|r_i]  \\
                                      &= \sum_{g\in G} \frac{P[r]}{n_{\ell_g}} \prod_{i=1}^{\ell_g} 4P[g_i|r_i]  \\
                                      &\approx \frac{P[r]}{n} \sum_{g\in G} \prod_{i=1}^{\ell_g} 4P[g_i|r_i]  \\
\end{split}
\end{equation*}

It is obvious that, the above equation requires computing the summation of products over the length $\ell_g$. This is where the iterative loops of data parallel operations were executed. Focusing on reducing the time required to complete the above task, we will describe the method adapted in the following chapter.

\chapter{Method}
\section{Method Overview}
In the original EAGLE publication, for all called variants, a hypothesis: instead of being identical to the reference genome $G_{ref}$, the individual providing the sequenced reads possesses an alternative version of human genome $G_{alt}$ consisting the called variant, is considered. The default hypothesis, assuming that the genome segment of the individual where the read is being sequenced from is identical to the reference genome, is also considered. In order to assess how well the above two hypotheses fits the sequenced read data $r$, the probability $P[r|G]$, for both $G_{ref}$ and $G_{alt}$, is measured. The ratio between the two would be obtained as the resulting quality score for the called variant constructing the alternative genome $G_{alt}$, revealing how confident we are that the called variant is actually present for the sequenced individual. As described in the previous chapter, the probabilities of all read $r$ and genome segment ${g\in G}$ pairs, $P[g|r]$, are calculated and then further summed up to acquire $P[r|G]$. Although this is undoubtedly a straightforward solution, numerous time consuming operations were done in the above operation, looping over all possible pairs of $r$ and $g$. Not to mention that such process should be repeated for all called variants, each of them considering every read that could map to related positions, twice for both $G_{ref}$ and the constructed $G_{alt}$. A brief pseudocode of the implementation demonstrating the looped structure is shown in Algorithm \ref{alg:loop}.
\begin{algorithm}
	\caption{Pseudocode of the implementation without parallelism}
	\label{alg:loop}
	\begin{algorithmic}
	% \Require $n \geq 0$
	\For{each $v\in \texttt{Called variants}$}
		\State $G_{ref} \gets \texttt{reference genome segment near } v$
		\State $G_{alt} \gets \texttt{genome segment consisting } v$
		\State $R \gets fetchReads()$ \Comment{Insert reads overlapping the position of $v$ into read set $R$}
		\For{each $r\in R$}
			\State $P[r|G_{ref}] \gets 0$
			\State $P[r|G_{alt}] \gets 0$
			\For{each $g\in G_{ref}$}
				\State $refProbability \gets 1$
				\For{$i=1$ to $\ell_r$}
					\State $refProbability \gets refProbability * P[r_i|g_i]$
				\EndFor
				\State $P[r|G_{ref}] \gets P[r|G_{ref}] + refProbability$
			\EndFor
			\For{each $g\in G_{alt}$}
				\State $altProbability \gets 1$
				\For{$i=1$ to $\ell_r$}
					\State $altProbability \gets altProbability * P[r_i|g_i]$
				\EndFor
				\State $P[r|G_{alt}] \gets P[r|G_{alt}] + altProbability$
			\EndFor
		\EndFor
	\EndFor
	\end{algorithmic}
\end{algorithm}
Apparently, computing the likelihood ratio for all called variants is time expensive when massive data is to be processed. Coming as a relief on the other hand, it is also obvious that the computation of $P[r|G]$ with different pairs of $r$ and $G$ (either $G_{ref}$ or $G_{alt}$), is independent of each other, enabling us to process them in parallel without worrying the orders they were computed. Here, we would like to take advantage of the high parallel graphics processing units and try to distribute the subtasks of computing $P[r|G_{ref}]$ and $P[r|G_{alt}]$, for every read-genome segment pair, to a separate GPU thread. Such process is visualized in Figure \ref{fig:overview}, where our goal is to parallelize the parts circled, with the attempt to reduce the total amount of time required to execute the program. We will go through the details of how does the CPU cooperate with the GPU and what a GPU thread is responsible for in the following section.

\begin{figure}
	\centering
	\includegraphics[scale=0.3]{figures/overview.png}
	\caption{Method overview}
	\label{fig:overview} % \ref{this label}
\end{figure}

\section{Proposed Scheme}
\subsection{CUDA programming model}
Before diving into the implementation details right away, we should first cautiously design our workflow, trying to acclimate to the CUDA programming model. In CUDA terminology, the CPU and memory in the system is often referred to as the host and the host memory, while the GPU being the device, maintaining its own device memory. With the GPU known for its high throughput facing parallel problems, the more generalized tasks such as processing user input is usually done first by the host, followed by the computationally expensive functions, or kernels in CUDA terminology, being executed on the device. Such mechanism is named as a kernel launch. In a programmer's perspective, we could freely determine how many threads to use for a certain task during the kernel launch phase. To better organize the numerous threads, CUDA threads are grouped into CUDA blocks, while multiple blocks further form a CUDA grid. Whenever a kernel is launched, a grid, consisting of blocks of threads, would be responsible for executing the kernel. Thus, for parallel tasks, we can write our own kernel functions to be executed in parallel, then specify the number of threads per block and the amount of blocks to be used in the grid when we launch the kernel. Though theoretically we can freely design kernels and select kernel launch options, there exists several essential points to be aware of, which could affect the performance significantly because of the hardware implementations of GPUs. For modern NVIDIA GPUs that support CUDA, they often come with numerous streaming processors(SP), which are the physical cores on the GPU. Each of them is responsible for executing a CUDA thread in our program. They are grouped to form streaming multiprocessors(SM), the basic unit programmers control, where a thread block is executed. Streaming multiprocessors have their own set of streaming processors, along with their own hardware units such as registers, cache, shared memory, and warp scheduler that schedules warps, the basic unit when it comes to runtime execution. Each warp contains 32 threads, while they share one program counter. Thus, all of the threads in the same warp would have to execute exact same instructions at a specific time. When threads in the same warp executes instructions conditionally, possibly through statements such as if-else, it will slow down the process since those threads not executing will be temporarily deactivated, waiting for others to finish. Such problem is referred to as warp divergence, and could result in a performance loss due to the serialized manner. Another issue that could worsen performance is memory copying. Since kernels launched on device cannot directly access the host memory, those data to be computed should be copied to the device beforehand. However, copying data between host and device is quite time consuming, being likely to take more time than the actual computing operations if not designed carefully. Hence, these are our main concerns during development, while seeking to keep the kernels generalized, ready for further integration. As a result, we designed the parallelized EAGLE workflow as shown in Algorithm \ref{alg:gpu}. First of all, the host reads user input, processes candidate variants and generates the alternative genome for each candidate variant exactly identical to the original method. Then, for each set of \{$r$, $G_{alt}$, $G_{ref}$\}, host calls CUDA api functions to allocate device memory space and copy data onto the device. After that, kernels are launched to calculate $P[r|G_{ref}]$ and $P[r|G_{alt}]$, with the results being copied back to host afterwards. Finally, the host unifies the results of each kernel launch and produces the quality score of a called variant. To better demonstrate how the task is designed in the host-device programming model, we briefly visualized the interaction between the host and device in Figure \ref{fig:prop}. 
\begin{algorithm}
	\caption{Pseudocode of the implementation with parallelism}
	\label{alg:gpu}
	\begin{algorithmic}
	% \Require $n \geq 0$
	\For{each $v\in \texttt{Called variants}$}
		\State $G_{ref} \gets \texttt{reference genome segment near } v$
		\State $G_{alt} \gets \texttt{genome segment consisting } v$
		\State $R \gets fetchReads()$ \Comment{Insert reads overlapping the position of $v$ into read set $R$}
		\For{each $r\in R$}
			\State $\texttt{allocateDeviceMemory()}$
			\State $\texttt{memoryCopyHostToDevice()}$
			\State $P[r|G_{ref}] \gets \texttt{calcLikelihoodKernel(r, $G_{ref}$)}$
			\State $P[r|G_{alt}] \gets \texttt{calcLikelihoodKernel(r, $G_{alt}$)}$
			\State $\texttt{memoryCopyDeviceToHost()}$
			\State $\texttt{releaseDeviceMemory()}$
		\EndFor
	\EndFor
	\end{algorithmic}
\end{algorithm}

\begin{figure}
	\centering
	\includegraphics[scale=0.8]{figures/proposed.png}
	\caption{Proposed workflow in host-device scheme}
	\label{fig:prop} % \ref{this label}
\end{figure}

After perceiving knowledge on the bigger picture of the proposed method scheme, we can now take a closer look on how kernels calculate $P[r|G]$ in a parallel fashion. We will then describe how we built up the parallelized implementation step by step in the following sections.

\subsection{Computing the likelihood with given genome segment}
First of all, for any given pair of genome segment $g$ and sequence read $r$ having the same length $\ell_r$, the likelihood of the read being sequenced from $g$ is computed by product of the likelihood of all base pairs, which is, 
\begin{equation}
	P[g|r] = \prod_{i=1}^{\ell_r} P[g_i|r_i]
\end{equation}
Since the likelihood of each base pair $(g_i, r_i)$ being a match/mismatch is independent to that of other base pairs, they can be computed concurrently. This provides us the opportunity to accelerate this process by computing them in parallel, where the maximum number of operations that can be done in parallel is bounded by the minimum value of: (1) length of the read $r$, or (2) the number of threads. Considering NGS read data, which is often around 100 to 150 base pairs in terms of read length, we can easily load the entire read onto the GPU and allocate a thread for each base, since the size is rather small in comparison to the memory capacity and the core counts of modern GPUs. That is, the $i$th thread being responsible for $P[g_i|r_i]$, for all $i \in \mathbb{N} , 1 \leq i \leq {\ell_r}$. On the other hand, if we take other sequencing technologies which might produce reads longer than the total amount of threads available into consideration, we can achieve similar result by letting the $i$th thread compute $P[g_{i+j}|r_{i+j}]$, for all $i \in \mathbb{N} ,1 \leq i \leq T$ and $j \in \mathbb{Z} ,0 \leq j, i+j \leq {\ell_r}$, with $T$ denoting the total amount of threads. A schematic diagram for such process is shown in Figure \ref{fig:prg}.
\begin{figure}
	\centering
	\includegraphics[scale=0.3]{figures/method1.png}
	\caption{Parallel computation for $P[g|r]$}
	\label{fig:prg} % \ref{this label}
\end{figure}

\subsection{Computing the likelihood with given reference genome}
Now we have $P[g|r]$ parallelized, we would like to further extend the degree of parallelism, seeking to utilize the performance provided by the GPU. Here, we take a closer look at the intermediate term, $P[r|G]$. In most cases, knowing the exact genome segment $g$ where read sequence $r$ is sequenced from in advance is unrealistic. Thus, in the original method, for all possible genome segment $g$ sampled from the hypothesis genome $G$ having a overlap with the called variant position, with the exact length ${\ell_r}$ as the given read $r$, were considered. Namely, we have to iteratively calculate $P[g|r], \forall g \in G$, before we add them up for the final result. This leads to at most ${\ell_r}$ possible $g$s to be processed. Despite the implementation described above, we would still have to launch the kernel calculating $P[g|r]$ as many times as the count of overlapping $g$s, at most ${\ell_r}$, to acquire a single result of $P[r|G]$.  Consequently, the execution time required grows in polynomial time with the sequence length. Similar to the concept when we implemented parallelized $P[g|r]$, though, it is apparent that the computation of $P[g|r]$ for different $g \in G$ is also independent. This opens up yet another opportunity for parallelism by using multiple blocks of threads, with each executing the kernel described in the previous section. In addition to the entire read $r$, we could also load the selected genome $G$ to the GPU instead of just a segment $g$. Subsequently, we allocate a block of threads for each possible genome segment $g$. 
That is, with $g_a$ denoting the $a$th possible genome segment with length ${\ell_r}$, we allocate the $j$th CUDA block to calculate $P[g_{j+k}|r]$, for all $k \in \mathbb{Z} ,0 \leq k, j+k \leq {\ell_r}$. Sush procedure is shown in Figure \ref{fig:prG}. Just like what we discussed in the previous section, with shorter read data, there is a high possibility that we have enough blocks to allocate exact one genome segment $g$ to a single block, and enough threads in each block to allocate a single sequence base to a thread. To better demonstrate this concept, the probability a thread needs to compute is illustrated in Figure \ref{fig:tv}, with $t_{jk}$ denoting the $k$th thread in the $j$th block, $r_i$ being the $i$th base of the read and $g_{ab}$ representing the $b$th base of the $a$th possible genome segment with length ${\ell_r}$. Having that in mind, it is easy now for us to scale up the solution for longer read data. For the general recursive expression, thread $t_{jk}$ should be responsible for $P[g_{k+NB,j+MT}|r_{j+MT}]$, where $B$ is the number of blocks launched and $T$ is the amount of threads per block, and that $N, M \in \mathbb{Z} ,0 \leq N, k+NB \leq {\ell_r}, 0 \leq M, j+MT \leq {\ell_r}$.
\begin{figure}
	\centering
	\includegraphics[scale=0.3]{figures/prG.png}
	\caption{Parallel computing for $P[r|G]$ using $j$ CUDA blocks}
	\label{fig:prG} % \ref{this label}
\end{figure}
\begin{figure}
	\centering
	\includegraphics[scale=0.3]{figures/threadview.png}
	\caption{Tasks for each thread when parallel computing $P[r|G]$}
	\label{fig:tv} % \ref{this label}
\end{figure}

\subsection{Revisiting the ignored: read indel errors}
Through the above method, we have already accelerated the execution of this program. This also allows us to revisit a term that was previously ignored: insertion/deletion, abbreviated as indel, errors in read sequences. Since indel errors not only modifies the sequence but also introduce shifts, it would increase the time complexity, proportional to the length of reads. With the previous implementation where the operations were sequentially executed, this would sure intensify the computational cost and lead to longer execution time. Mainly considering NGS data, which has a rather low probability of indel errors in the sequencing phase, an assumption that there was no indel sequencing errors has been made, in order to avoid the problem described above. However, with the aid of GPUs, such operations can be accelerated if done concurrently, reducing the amount of time required to compute the final result. In the EAGLE-GPU implementation, we introduced a threshold value that could be given by the user, indicating the maximum total length of indel errors to be taken into account. After all, the probability of such errors is still fairly low, making it pointless to compute all of the possible combinations of shifted reads and indels. Knowing the threshold beforehand, all of the possible reads having shorter indel errors than the threshold value could be listed out and assigned to the GPU. In other words, during the calculation of $P[r|G]$, we sum up all results of $P[r'|G], \forall r' \in R_{indel}$, where $R_{indel}$ is the list containing all possible combinations of read $r$ having indels shorter than the threshold value. Similar to the base case where no indel errors was considered, we would need a block of threads for each pair of read $r'$ and genome segment $g$. Fortunately, in the CUDA api, programmers are allowed to launch thread blocks up to three dimensional. That is, instead of the original CUDA grid with one dimensional blocks, we could launch with two dimensional blocks, with the value of the extra dimension representing the index of read in $R_{indel}$. When it comes to scalability, with the attempt to allow the length of $R_{indel}$ to exceed the maximum value of the second block dimension, the list $R_{indel}$ could first be partitioned to batches, each limited to the maximum value of the extra dimension. After that, the results of each batch would be added up to acquire the final result. Although it is beneficial if we would like to consider a few indels, this implementation is still rather straightforward and might face difficulties in real world applications, which would be further discussed in the next two chapters.

\chapter{Results}
\section{Input dataset}
To better compare the results with and without GPU acceleration, an experiment was conducted using the reconstructed diploid sequence of human genome chromosome 22 from hg19 reference sequence, which is one of the dataset that were used in the original study. Intend to investigate possible speed-up when facing third generation sequencing read data, simulated sequencing reads with 10,000 base pairs were generated by random sampling from the reference genome. Reads with different lengths were also generated as a reference and an indicator for possible future generation sequencers.

\section{Benchmark platforms}
EAGLE-GPU is compiled with CUDA version 10.1. The experiments were conducted on a machine with a single AMD Ryzen™ 9 5950X desktop processor, consisting of 16 hyper-threaded physical cores running at their base clock 3.4 gigahertz, and 128 GB of random access memory. The GPU installed on the machine is a single NVIDIA GeForce RTX™ 3090 graphics card.

\section{Identical dataset: The NS12911 variants}
First of all, we used the exact same variant dataset that was mentioned in the original EAGLE paper to inspect the performance difference between the sequential and the parallelized implementations. In search of the best grid size and block size settings for this task, we conducted a first experiment disabling the CPU's multithreading setting, measuring the time taken for all combinations of 1, 32, 64, 128, 256, threads and blocks respectively. The measurements are shown in Table \ref{table:1}. As we can see in Figure \ref{fig:gpuconfig} where we visualized the results, the best configuration would be using 32 blocks per grid, with each block running 32 threads. This results in the program taking 0.342212 hours to finish the task. Later on, attempting to compare the results with and without GPU acceleration, the experiment is then conducted by adopting a single threaded CPU setting, with and without GPU parallel computing under the best kernel launch configuration, gradually moving to a multithreaded setting. The results are shown in Figure \ref{fig:multithread}. For this dataset, the execution time both reduced with the amount of threads increasing, until 8 threads were used. However, we can see that with the current implementation of GPU computing, the execution time is not effectively reduced, but is somehow worsened instead.
\begin{table}[h!]
	\centering
	\begin{tabular}{l*{5}{c}}
		Block size              & 1 & 32 & 64 & 128 & 256\\
		\hline
		1 block & 0.363213 & 0.351147 & 0.350840 & 0.349051 & 0.346477  \\
		32 blocks            & 0.364184 & 0.342212 & 0.345006 & 0.344206 & 0.344719  \\
		64 blocks          & 0.353136 & 0.353382 & 0.347007 & 0.349023 & 0.345956  \\
		128 blocks     & 0.354095 & 0.353936 & 0.352839 & 0.349773 & 0.349343  \\
		256 blocks     & 0.351260 & 0.355450 & 0.359848 & 0.361534 & 0.357256  \\
	\end{tabular}
	\caption{Time taken (in hours) for different kernel launch configuration}
	\label{table:1}
\end{table}

\begin{figure}
	\centering
	\includegraphics[scale=0.3]{figures/gpu_config.png}
	\caption{Time taken for different kernel launch configuration}
	\label{fig:gpuconfig} 
\end{figure}
\begin{figure}
	\centering
	\includegraphics[scale=0.3]{figures/multithread.png}
	\caption{Time taken for different CPU threading settings}
	\label{fig:multithread} 
\end{figure}

\section{Simulated reads with different lengths}
With the length of the sequenced reads growing considering third generation sequencing, we performed another experiment, with the intention to figure out the relations between parallelism and sequence length. We did not only test with sequences with around 10000 bp which is the most common length for Nanopore sequencing results, but also other varying lengths, preparing our tool in advance for possible future sequencing technologies which might produce reads in varying lengths. Simulated reads were generated, with different lengths, by randomly sampling from the reference genome. Concentrating on the performance difference regarding the presence of GPU acceleration only, we performed this experiment in a single CPU threading setting, with 256 blocks x 256 threads kernel launch configuration. Clearly seen in Figure \ref{fig:readlen}, the execution time without GPU acceleration dramatically increases with the read length, while that of GPU accelerated version growing rather mild, outperforming the other after reads expand over 1000 base pairs.
\begin{figure}
	\centering
	\includegraphics[scale=0.3]{figures/readlen.png}
	\caption{Execution time for different input read lengths}
	\label{fig:readlen} 
\end{figure}

\section{Considering read indel errors}
Attempting to find out if the consideration of indel errors is feasible, although computationally expensive, with the assistance of GPU parallel computing, we reused the NS12911 dataset, but without any assumption about the presence of read indel errors. Instead, a threshold is issued, while all of the reads with indel errors length within the threshold is considered. As mentioned in the previous study, doing so with CPU only is extremely time-consuming, thus we had to set a limitation for the threshold, not exceeding 1, in order to regulate the total execution time to a reasonable extent. As we can see in Figure \ref{fig:readindel}, a positive result of GPU speed-up is achieved even with one possible read indel error, not to mention computing even more combinations of indels for a larger threshold.
\begin{figure}
	\centering
	\includegraphics[scale=0.3]{figures/read_indel.png}
	\caption{Time taken for different indel threshold settings}
	\label{fig:readindel} 
\end{figure}

\chapter{Discussion}
EAGLE-GPU is a GPU accelerated implementation of the previously published method, EAGLE, while maintaining similar data processing workflow for better integration with the previous tool. Being generalized by re-engineering the fundamental functions, not packing the entire algorithm into a hard coded GPU kernel sure helps with flexibility, preparing the tool for further applications. However, performance were sacrificed to a certain extent by doing so. In the following sections, we would like to discuss our interpretation of the experimental results, along with possible approaches to further increase the speed-up.

\section{Case study: The original dataset}
As mentioned in the previous chapter, we can easily see that applying GPU computation does not benefit the results when the dataset tested in the previous research was used. Through observations of the profiling results, it is clear that the computational time, excluding the time for data copying between host and device, is longer for GPUs, not to mention data copying adds up to the total execution time if GPU is used. The main reason for this is that, although equipped with a lot more processors, the computational power of a single GPU core is weaker than that of a CPU core. As a consequence, if the extent of parallelism was not high enough, distributing tasks to the GPU could possibly become a drawback when it comes to execution time. To better illustrate such problem, a rather easy experiment was made, which is the addition of two vectors, with the execution time measured for different vector sizes. The results are shown in Figure \ref{fig:cpugpuvec}.
\begin{figure}
	\centering
	\includegraphics[scale=0.3]{figures/cpugpu_vector.png}
	\caption{Comparing the performance between CPU and GPU}
	\label{fig:cpugpuvec} % \ref{this label}
\end{figure}
As we can see in Figure \ref{fig:cpugpuvec}, applying GPU acceleration is not favoring until the size of vector exceeds 800000. Now we further inspect our dataset, where all of the sequencing reads have a length of 100 bp. When we calculate $P[r|G]$, a total of 100 x 100 = 10000 operations can be executed concurrently, each computing the probability at a certain cite. Although the time taken between addition and querying the probability of a DNA base might differ, but the considerable difference between the two numbers leads us to infer that the reads in this specific dataset is too short for it to be profitable when it comes to GPU computation. Such supposition could be confirmed with the second experiment, which we would further discuss in the next section. As a result, we suggest not to apply GPU acceleration when handling datasets with shorter reads. Since the instructions performed recurrently are rather little, the CPU can effectively finish the job solely, without needing any help from the GPU. The only condition we could think of that GPU acceleration would be profitable with short reads is when numerous reads maps to a single interval of interest. Under this circumstance, implementing a specialized kernel, computing $P[R|G]$, where $R$ denotes the set of reads, could be a practical approach. However, the main objective for this research is to build a rather general solution, for future applications to build on, thus this will be left as a possible future work, which can be conducted in a similar fashion if such type of data is encountered.

\section{Case study: Simulated TGS read data}
By conducting this experiment, we could not only validate the presumption described in the previous section, but also acquire an estimation of the possible speed-up when applying EAGLE-GPU to third generation sequencing data. Here, we can easily see that the speed-up performance gradually increases as the size of sequenced reads grow, surpassing it of the CPU when the read length passes 1000 bp. This sure supports our presumption, concluding that GPU acceleration is a favorable option when it comes to third generation sequencing, or even further sequencing technologies with sequencing read lengths beyond 1000 bp.

\section{Case study: Read indel errors}
The execution time dramatically increases when read indels are taken into consideration, justifying the attempt to ignore such errors in the previous work. With the aid of GPUs, such difficulty could be tackled, to a certain extent. As long as it fits into the device memory, we can load all possible reads, including different combinations of indels, onto the GPU. They can be computed concurrently, greatly reducing the time required for such operations, despite being computationally expensive. With the threshold adjustable, GPU acceleration could come in as a relief if the sequencing technology is reported to have a noticeable read indel issue. However, the main drawback of this method would be the time needed to find $R_{indel}$, which consists of all possible combinations of read $r$ having indels no longer than the given threshold value. Our current implementation definitely works fine with shorter reads and a rather low threshold value, but when it comes to upsurging read lengths or sequencing technologies that introduce long read indel errors, it would nonetheless be time consuming just to list out $R_{indel}$, despite the computation of $P[r'|G], \forall r' \in R_{indel}$ being accelerated. Thus, we recommend trying our GPU implementation for NGS data, where sequenced reads are comparatively short with lower probabilities of read indel errors, while the task of optimizing such method for TGS data analysis left for future work.

\section{Future Work}
Trying to investigate various possibilities of application for GPU to improve the performance, this paper provides the implementations of the general usage functions in CUDA. Unlike programs executed particularly on CPU, the design of GPU kernels often require extra attention, including memory copying schema and the detail implementation of algorithms to avoid warp divergence. If we purely consider datasets with shorter sequencing reads, the extent of parallelism could be further increased by computing different read sets concurrently. With a specific kernel for each case, warp divergence could be eliminated. Theoretically, this could benefit cases when the dataset consist of large amount of read sets, enormously reducing the amount of time to evaluate the likelihood by computing the results for each set all at once. Another possible optimization would be lessening the memory copying cost. Despite it not being the bottleneck of the program, some advanced techniques could be applied to lower the amount of data copied between the host CPU and the device even more. For example, if we are targeting only DNA and RNA sequences but not proteins, we could easily represent the bases, A, C, G, T(U for RNA), N, in 3 bits. Thus, instead of copying them as characters, they could be encoded first, enabling us to pack 2 bases into one unsigned integer. This is a common trick among sequencing related GPU applications, also adopted by other tools such as GASAL2~\cite{ahmed2019gasal2} and NVBIO~\cite{nvbio2015}.

\chapter{Conclusion}
In this paper, we propose and implemented a GPU accelerated version of EAGLE, a previously proposed method for alternative genome likelihood evaluation. GPU-EAGLE utilizes the massively parallel architecture of modern GPUs, enabling instructions to be executed in parallel on multiple different data. This prepares the original method for the continuous growth when it comes to the size of sequencing data. Besides, we also investigated the feasibility of taking sequencing read indel errors into consideration, which is computational expensive and used to be ignored, after increasing the program throughput by applying parallel computation. We provide experimental results showing that GPU-EAGLE outperforms the CPU version when the data size is large enough, and conclude that designing a specialized GPU kernel could always be an option for forthcoming applications with excessive computational operations.

\newpage
\AddToContents{Bibliography}
\printbibliography


\end{document}
